<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Pytorch 基本概念 | Sceliay's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Pytorch 基本概念</h1><a id="logo" href="/.">Sceliay's Blog</a><p class="description">Welcome to my blog!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Pytorch 基本概念</h1><div class="post-meta">2019-12-09<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><a class="disqus-comment-count" href="/2019/12/09/pytorch/#vcomment"><span class="valine-comment-count" data-xid="/2019/12/09/pytorch/"></span><span> 条评论</span></a><div class="post-content"><p>参考：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/PyTorch-introduction.ipynb">code of learn deep learning with pytroch</a>，<a href="https://pytorch-cn.readthedocs.io/zh/latest/notes/autograd/" target="_blank" rel="noopener">PyTorch中文文档</a></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>conda: <code>conda install pytorch torchvision -c pytorch</code><br>pip: <code>pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl</code>, <code>pip install torchvision</code><br><a href="https://pytorch.org/" target="_blank" rel="noopener">官网</a></p>
<h1 id="Tensor-amp-Variable"><a href="#Tensor-amp-Variable" class="headerlink" title="Tensor &amp; Variable"></a>Tensor &amp; Variable</h1><ol>
<li><p>Pytorch 是一个拥有强力GPU加速的张量和动态构建网络的库，其主要构建是张量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 创建 numpy ndarray</span><br><span class="line">numpy_tensor = np.random.randn(10,20)</span><br><span class="line"></span><br><span class="line"># 创建 pytorch tensor</span><br><span class="line">pytorch_tensor = torch.randn(3,2)</span><br><span class="line"></span><br><span class="line"># 将 ndarray 转换到 tensor 上</span><br><span class="line">pytorch_tensor1 = torch.Tensor(numpy_tensor)</span><br><span class="line">pytorch_tensor2 = torch.from_numpy(numpy_tensor)</span><br><span class="line"></span><br><span class="line"># 将 tensor 转换为 ndarray</span><br><span class="line"># cpu</span><br><span class="line">numpy_array = pytorch_tensor1.numpy()</span><br><span class="line"># gpu</span><br><span class="line">numpy_array = pytorch_tensor1.cpu().numpy()</span><br></pre></td></tr></table></figure>
</li>
<li><p>将 tensor 放到 GPU 上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 第一种方式：定义 cuda 数据类型</span><br><span class="line">dtype = torch.cuda.FloatTensor # 定义默认 GPU 的 数据类型</span><br><span class="line">gpu_tensor = torch.randn(10, 20).type(dtype)</span><br><span class="line"></span><br><span class="line"># 第二种方式：</span><br><span class="line">gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上</span><br><span class="line">gpu_tensor = torch.randn(10, 20).cuda(1) # 将 tensor 放到第二个 GPU 上</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>使用第一种方式将 tensor 放到 GPU 上的时候会将数据类型转换成定义的类型，而是用第二种方式能够直接将 tensor 放到 GPU 上，类型跟之前保持一致</p>
<ol>
<li><p>将 tensor 放回 CPU</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpu_tensor = gpu_tensor.cpu()</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问 tensor 的属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 大小</span><br><span class="line">pytorch_tensor1.shape</span><br><span class="line">pytorch_tensor1.size()</span><br><span class="line"></span><br><span class="line"># 类型</span><br><span class="line">pytorch_tensor1.type()</span><br><span class="line">pytorch_tensor1.type(torch.DoubleTensor) #转换为float64</span><br><span class="line"></span><br><span class="line"># 维度</span><br><span class="line">pytorch_tensor1.dim()</span><br><span class="line"></span><br><span class="line"># 所有元素个数</span><br><span class="line">pytorch_tensor1.numel()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Tensor 操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(2,2) # float tensor</span><br><span class="line"></span><br><span class="line">x = x.long() # 转换为整型</span><br><span class="line">x = x.float() # 转换为 float</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.randn(4,3) # 随机矩阵</span><br><span class="line"></span><br><span class="line"># 沿行取最大值，返回每一行最大值及下标</span><br><span class="line">max_value, max_index = torch.max(x, dim=1) </span><br><span class="line"></span><br><span class="line"># 沿行求和</span><br><span class="line">sum_x = torch.sum(x, dim=1)</span><br><span class="line"></span><br><span class="line"># 增加维度</span><br><span class="line">x = x.unsqueeze(0) # 在第一维加</span><br><span class="line"></span><br><span class="line"># 减少维度</span><br><span class="line">x = x.squeeze(0) # 减少第一维</span><br><span class="line">x = x.squeeze() # 将 tensor 中所有一维去掉</span><br><span class="line"></span><br><span class="line"># 维度交换</span><br><span class="line">x = x.permute(1,0,2) # 重新排列 tensor 的维度</span><br><span class="line">x = x.transpose(0,2) # 交换 tensor 中的两个维度</span><br><span class="line"></span><br><span class="line"># 使用 view 对 tensor 进行 reshape</span><br><span class="line">x = torch.randn(3,4,5)</span><br><span class="line">x = x.view(-1,5) # -1 表示任意大小，5表示第二维变成5</span><br><span class="line"></span><br><span class="line"># 两个 tensor 求和</span><br><span class="line">x = torch.randn(3,4)</span><br><span class="line">y = torch.randn(3,4)</span><br><span class="line"></span><br><span class="line">z = x+y</span><br><span class="line">z = torch.add(x,y)</span><br><span class="line"></span><br><span class="line"># inplace 操作，在操作的符号后加_</span><br><span class="line">x.unsqueeze_(0)</span><br><span class="line">x.transpose_(1,0)</span><br><span class="line">x.add_(y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Variable<br>Variable 是对 tensor 的封装，包含三个属性：<code>.data</code> tensor 本身，<code>.grad</code> tensor 的梯度，<code>.grad_fn</code> variable 的获得方式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">x_tensor = torch.randn(10, 5)</span><br><span class="line">y_tensor = torch.randn(10, 5)</span><br><span class="line"></span><br><span class="line"># 将 tensor 变成 Variable</span><br><span class="line">x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度</span><br><span class="line">y = Variable(y_tensor, requires_grad=True)</span><br><span class="line"></span><br><span class="line">z = torch.sum(x + y)</span><br><span class="line"></span><br><span class="line">print(z.data)</span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"># 求 x 和 y 的梯度</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line">print(y.grad)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><ol>
<li><p>简单情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">x = Variable(torch.Tensor([2]), requires_grad=True)</span><br><span class="line">y = x + 2</span><br><span class="line">z = y ** 2 + 3</span><br><span class="line"></span><br><span class="line">z.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
</li>
<li><p>复杂情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True) # 构建一个 1 x 2 的矩阵</span><br><span class="line">n = Variable(torch.zeros(1, 2)) # 构建一个相同大小的 0 矩阵</span><br><span class="line"></span><br><span class="line">n[0, 0] = m[0, 0] ** 2</span><br><span class="line">n[0, 1] = m[0, 1] ** 3</span><br><span class="line"></span><br><span class="line">n.backward(torch.ones_like(n)) # 将 (w0, w1) 取成 (1, 1)</span><br><span class="line">print(m.grad)</span><br></pre></td></tr></table></figure>
</li>
<li><p>多次自动求导</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.FloatTensor([3]), requires_grad=True)</span><br><span class="line">y = x * 2 + x ** 2 + 3</span><br><span class="line"></span><br><span class="line">y.backward(retain_graph=True) # 设置 retain_graph 为 True 来保留计算图</span><br><span class="line">print(x.grad) # 8</span><br><span class="line"></span><br><span class="line">y.backward() # 再做一次自动求导，这次不保留计算图</span><br><span class="line">print(x.grad) # 16</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这里做了两次自动求导，16 为第一次的梯度 8 和第二次的梯度 8 加和结果。</p>
<ol>
<li>练习<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.FloatTensor([2, 3]), requires_grad=True)</span><br><span class="line">k = Variable(torch.zeros(2))</span><br><span class="line"></span><br><span class="line">k[0] = x[0] ** 2 + 3 * x[1]</span><br><span class="line">k[1] = x[1] ** 2 + 2 * x[0]</span><br><span class="line"></span><br><span class="line">j = torch.zeros(2, 2)</span><br><span class="line"></span><br><span class="line">k.backward(torch.FloatTensor([1, 0]), retain_graph=True)</span><br><span class="line">j[0] = x.grad.data</span><br><span class="line"></span><br><span class="line">x.grad.data.zero_() # 归零之前求得的梯度</span><br><span class="line"></span><br><span class="line">k.backward(torch.FloatTensor([0, 1]))</span><br><span class="line">j[1] = x.grad.data</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="线性模型和梯度下降"><a href="#线性模型和梯度下降" class="headerlink" title="线性模型和梯度下降"></a>线性模型和梯度下降</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">torch.manual_seed(2017)</span><br><span class="line"></span><br><span class="line"># 读入数据 x 和 y</span><br><span class="line">x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],</span><br><span class="line">                    [9.779], [6.182], [7.59], [2.167], [7.042],</span><br><span class="line">                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],</span><br><span class="line">                    [3.366], [2.596], [2.53], [1.221], [2.827],</span><br><span class="line">                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line"># 画出图像</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.plot(x_train, y_train, &apos;bo&apos;)</span><br><span class="line"></span><br><span class="line"># 转换成 Tensor</span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"></span><br><span class="line"># 定义参数 w 和 b</span><br><span class="line">w = Variable(torch.randn(1), requires_grad=True) # 随机初始化</span><br><span class="line">b = Variable(torch.zeros(1), requires_grad=True) # 使用 0 进行初始化</span><br><span class="line"></span><br><span class="line"># 构建线性回归模型</span><br><span class="line">x_train = Variable(x_train)</span><br><span class="line">y_train = Variable(y_train)</span><br><span class="line"></span><br><span class="line">def linear_model(x):</span><br><span class="line">    return x * w + b</span><br><span class="line"></span><br><span class="line">y_ = linear_model(x_train)</span><br><span class="line"></span><br><span class="line"># 模型输出</span><br><span class="line">plt.plot(x_train.data.numpy(), y_train.data.numpy(), &apos;bo&apos;, label=&apos;real&apos;)</span><br><span class="line">plt.plot(x_train.data.numpy(), y_.data.numpy(), &apos;ro&apos;, label=&apos;estimated&apos;)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"># 计算误差</span><br><span class="line">def get_loss(y_, y):</span><br><span class="line">    return torch.mean((y_ - y_train) ** 2)</span><br><span class="line"></span><br><span class="line">loss = get_loss(y_, y_train)</span><br><span class="line"></span><br><span class="line"># 自动求导</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"># 更新一次参数</span><br><span class="line">w.data = w.data - 1e-2 * w.grad.data</span><br><span class="line">b.data = b.data - 1e-2 * b.grad.data</span><br><span class="line"></span><br><span class="line"># 进行 10 次更新</span><br><span class="line">for e in range(10):</span><br><span class="line">    y_ = linear_model(x_train)</span><br><span class="line">    loss = get_loss(y_, y_train)</span><br><span class="line">    </span><br><span class="line">    w.grad.zero_() # 记得归零梯度</span><br><span class="line">    b.grad.zero_() # 记得归零梯度</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    w.data = w.data - 1e-2 * w.grad.data # 更新 w</span><br><span class="line">    b.data = b.data - 1e-2 * b.grad.data # 更新 b </span><br><span class="line">    print(&apos;epoch: &#123;&#125;, loss: &#123;&#125;&apos;.format(e, loss.data[0]))</span><br></pre></td></tr></table></figure>
<ol>
<li><p>获得 <code>[x,x^2,x^3]</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_sample = np.arange(-3, 3.1, 0.1)</span><br><span class="line">x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>线性模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def multi_linear(x):</span><br><span class="line">    return torch.mm(x, w) + b</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 设定随机种子</span><br><span class="line">torch.manual_seed(2017)</span><br><span class="line"></span><br><span class="line"># 从 data.txt 中读入点</span><br><span class="line">with open(&apos;./data.txt&apos;, &apos;r&apos;) as f:</span><br><span class="line">    data_list = [i.split(&apos;\n&apos;)[0].split(&apos;,&apos;) for i in f.readlines()]</span><br><span class="line">    data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]</span><br><span class="line"></span><br><span class="line"># 标准化</span><br><span class="line">x0_max = max([i[0] for i in data])</span><br><span class="line">x1_max = max([i[1] for i in data])</span><br><span class="line">data = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]</span><br><span class="line"></span><br><span class="line">x0 = list(filter(lambda x: x[-1] == 0.0, data)) # 选择第一类的点</span><br><span class="line">x1 = list(filter(lambda x: x[-1] == 1.0, data)) # 选择第二类的点</span><br><span class="line"></span><br><span class="line">np_data = np.array(data, dtype=&apos;float32&apos;) # 转换成 numpy array</span><br><span class="line">x_data = torch.from_numpy(np_data[:, 0:2]) # 转换成 Tensor, 大小是 [100, 2]</span><br><span class="line">y_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # 转换成 Tensor，大小是 [100, 1]</span><br><span class="line"></span><br><span class="line"># 定义 sigmoid 函数</span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1 / (1 + np.exp(-x))</span><br><span class="line"></span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"># 计算loss</span><br><span class="line">def binary_loss(y_pred, y):</span><br><span class="line">    logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean() # clamp 约束最大值最小值</span><br><span class="line">    return -logits</span><br><span class="line"></span><br><span class="line"># 使用 torch.optim 更新参数</span><br><span class="line">from torch import nn</span><br><span class="line">w = nn.Parameter(torch.randn(2, 1))</span><br><span class="line">b = nn.Parameter(torch.zeros(1))</span><br><span class="line"></span><br><span class="line">def logistic_regression(x):</span><br><span class="line">    return F.sigmoid(torch.mm(x, w) + b)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=1.)</span><br><span class="line"></span><br><span class="line"># 进行 1000 次更新</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">for e in range(1000):</span><br><span class="line">    # 前向传播</span><br><span class="line">    y_pred = logistic_regression(x_data)</span><br><span class="line">    loss = binary_loss(y_pred, y_data) # 计算 loss</span><br><span class="line">    # 反向传播</span><br><span class="line">    optimizer.zero_grad() # 使用优化器将梯度归 0</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step() # 使用优化器来更新参数</span><br><span class="line">    # 计算正确率</span><br><span class="line">    mask = y_pred.ge(0.5).float()</span><br><span class="line">    acc = (mask == y_data).sum().data[0] / y_data.shape[0]</span><br><span class="line">    if (e + 1) % 200 == 0:</span><br><span class="line">        print(&apos;epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;&apos;.format(e+1, loss.data[0], acc))</span><br><span class="line">during = time.time() - start</span><br><span class="line">print()</span><br><span class="line">print(&apos;During Time: &#123;:.3f&#125; s&apos;.format(during))</span><br></pre></td></tr></table></figure>
<ul>
<li>pytorch 中包含一些常见 <a href="https://pytorch.org/docs/0.3.0/nn.html#loss-functions" target="_blank" rel="noopener">loss</a>，如线性回归分类<code>nn.MSE()</code>和二分类<code>nn.BCEWithLogitsLoss()</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 使用自带的loss</span><br><span class="line">criterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性</span><br><span class="line"></span><br><span class="line">w = nn.Parameter(torch.randn(2, 1))</span><br><span class="line">b = nn.Parameter(torch.zeros(1))</span><br><span class="line"></span><br><span class="line">def logistic_reg(x):</span><br><span class="line">    return torch.mm(x, w) + b</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([w, b], 1.)</span><br><span class="line"></span><br><span class="line">y_pred = logistic_reg(x_data)</span><br><span class="line">loss = criterion(y_pred, y_data)</span><br><span class="line">print(loss.data)</span><br><span class="line"></span><br><span class="line"># 同样进行 1000 次更新</span><br><span class="line">start = time.time()</span><br><span class="line">for e in range(1000):</span><br><span class="line">    # 前向传播</span><br><span class="line">    y_pred = logistic_reg(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    # 反向传播</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    # 计算正确率</span><br><span class="line">    mask = y_pred.ge(0.5).float()</span><br><span class="line">    acc = (mask == y_data).sum().data / y_data.shape[0]</span><br><span class="line">    if (e + 1) % 200 == 0:</span><br><span class="line">        print(&apos;epoch: &#123;&#125;, Loss: &#123;:.5f&#125;, Acc: &#123;:.5f&#125;&apos;.format(e+1, loss.data, acc))</span><br><span class="line"></span><br><span class="line">during = time.time() - start</span><br><span class="line">print()</span><br><span class="line">print(&apos;During Time: &#123;:.3f&#125; s&apos;.format(during))</span><br></pre></td></tr></table></figure></li>
</ul>
</div><div class="tags"><a href="/tags/Python"><i class="fa fa-tag">Python</i></a><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a><a href="/tags/Pytorch"><i class="fa fa-tag">Pytorch</i></a></div><div class="post-nav"><a class="pre" href="/2019/12/17/pytorch2/">Pytorch 模型</a><a class="next" href="/2019/12/04/attention/">Attention</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'AMlVKrsPTMdYDOomfWIdGiFC-gzGzoHsz',
  appKey:'Msy2N1GDdBCf0W3MY9fjRYHc',
  placeholder:'Comments',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>当你凝视深渊的时候，深渊也在凝视着你。</p><a class="info-icon" href="https://twitter.com/yren_zj" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure/">Data Structure</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML4Med/">ML4Med</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/EHR/" style="font-size: 15px;">EHR</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/26/med-bert/">Med BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/18/mldl/">Lable Distribution for Multimodal Machine Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/cat/">cat</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/13/keras/">Keras 学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/20/BERT/">BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/26/offer/">剑指offer</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/23/matplotlib/">Matplotlib绘图</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/22/ai4md/">AI for Medicine</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/01/ml/">Machine Learning 面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/30/tensorflow-2-0/">tensorflow-2.x</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Sceliay's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>