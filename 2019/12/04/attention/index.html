<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Attention | Sceliay's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Attention</h1><a id="logo" href="/.">Sceliay's Blog</a><p class="description">Welcome to my blog!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Attention</h1><div class="post-meta">2019-12-04<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><a class="disqus-comment-count" href="/2019/12/04/attention/#vcomment"><span class="valine-comment-count" data-xid="/2019/12/04/attention/"></span><span> 条评论</span></a><div class="post-content"><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p>参考：<a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" target="_blank" rel="noopener">Attn: Illustrated Attention</a></p>
<p>随着深度学习的发展，Neural Machine Translation(NMT) 也逐渐取代传统的 Statistical Machine Translation(SMT)。 其中，最广为人知的框架为 <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sutskever et. al</a> 提出的 sequence-to-sequence(seq2seq) 模型。</p>
<p>Seq2seq 是一个含有两个 RNN 的 encoder-decoder 模型： encoder，按字符读入从而获得一个固定长度的表示； decoder,根据这些输入训练另一个 RNN 从而获得顺序输出。</p>
<p>这样的模型会导致最终 decoder 获得的是 encoder 最后输出的 hidden state，当文本过长时，容易遗忘开始输入的字段。</p>
<p>Attention 可以作为 encoder 与 decoder 的中间接口，为 decoder 提供每一个 encoder hidden state 的信息。如此，模型能够选择性的关注有用的部分，并学习 encoder 与 decoder 中字句的对齐。</p>
<p>Attention 有两种类型： global attention 使用所有的 encoder hidden state，local attention 只使用部分。 Attention layer 的实现可分为4个步骤。</p>
<ul>
<li><p>准备 hidden states.<br>例子中包含4个 encoder hidden states 和 current decoder hidden state.<br>Note: 最后一个 encoder hidden state 作为 decoder 的第一个 time step 输入。 第一个 time step 的输出被称为第一个 decoder hidden state.</p>
</li>
<li><p>计算每个 encoder hidden state 的 score.<br>可以通过一个 score function（也被称为 alignment score function 或 alignment model）来计算 score。在本例中，score function 为 dot product。更多 <a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#ba24" target="_blank" rel="noopener">score function</a>。<br>计算公式：<code>score = decoder hidden state x encoder hidden state</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">decoder_hidden = [10, 5, 10]</span><br><span class="line">encoder_hidden  score</span><br><span class="line">---------------------</span><br><span class="line">     [0, 1, 1]     15 (= 10×0 + 5×1 + 10×1, the dot product)</span><br><span class="line">     [5, 0, 1]     60</span><br><span class="line">     [1, 1, 0]     15</span><br><span class="line">     [0, 5, 1]     35</span><br></pre></td></tr></table></figure>
</li>
<li><p>将所有 scores 通过一个 softmax 层。<br>通过 softmax 层，将所有 softmaxed scores 加和为1，被称为 attention distribution.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder_hidden  score  score^</span><br><span class="line">-----------------------------</span><br><span class="line">     [0, 1, 1]     15       0</span><br><span class="line">     [5, 0, 1]     60       1</span><br><span class="line">     [1, 1, 0]     15       0</span><br><span class="line">     [0, 5, 1]     35       0</span><br></pre></td></tr></table></figure>
</li>
<li><p>将每个 encoder hidden state 乘以对应 softmaxed score.<br>通过 encoder hidden state 乘以相应 softmaxed score，可以获得 alignment vector 或 annotation vector。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder  score  score^  alignment</span><br><span class="line">---------------------------------</span><br><span class="line">[0, 1, 1]   15      0   [0, 0, 0]</span><br><span class="line">[5, 0, 1]   60      1   [5, 0, 1]</span><br><span class="line">[1, 1, 0]   15      0   [0, 0, 0]</span><br><span class="line">[0, 5, 1]   35      0   [0, 0, 0]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这里，表示第一个翻译的词与嵌入<code>[5,0,1]</code>的输入相对应。</p>
<ul>
<li><p>将 alignment vectors 相加。<br>将 alignment vectors 相加获得 context vector。</p>
</li>
<li><p>将 context vector 输入 decoder.</p>
</li>
</ul>
<h1 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h1><p>参考：<a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank" rel="noopener">Illustrated: Self-Attention</a></p>
<ul>
<li><p>输入<br>例子中，输入为3个4维向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input 1: [1, 0, 1, 0] </span><br><span class="line">Input 2: [0, 2, 0, 2]</span><br><span class="line">Input 3: [1, 1, 1, 1]</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化权值<br>每个输入有三个表示：key, query, value。例子中，这些表示用3维向量表示，则权值为4*3矩阵。<br>Note: value的维度与输出相同。</p>
<ul>
<li><p>key 的权值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0, 0, 1],</span><br><span class="line"> [1, 1, 0],</span><br><span class="line"> [0, 1, 0],</span><br><span class="line"> [1, 1, 0]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>query 的权值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1, 0, 1],</span><br><span class="line"> [1, 0, 0],</span><br><span class="line"> [0, 0, 1],</span><br><span class="line"> [0, 1, 1]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>value 的权值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0, 2, 0],</span><br><span class="line"> [0, 3, 0],</span><br><span class="line"> [1, 0, 3],</span><br><span class="line"> [1, 1, 0]]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Nots: 在神经网络中，权值常由合适的随机分布来初始化，如 Gaussian, Xavier 和 Kaiming 分布。</p>
</li>
<li><p>计算 key, query 和 value<br>计算公式为：<code>input x weight</code></p>
<ul>
<li><p>key:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">               [0, 0, 1]</span><br><span class="line">[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]</span><br><span class="line">[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]</span><br><span class="line">[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]</span><br></pre></td></tr></table></figure>
</li>
<li><p>query:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">               [1, 0, 1]</span><br><span class="line">[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]</span><br><span class="line">[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]</span><br><span class="line">[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]</span><br></pre></td></tr></table></figure>
</li>
<li><p>value:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">               [0, 2, 0]</span><br><span class="line">[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] </span><br><span class="line">[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]</span><br><span class="line">[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Note: 有时候也可以加上偏置。</p>
</li>
<li><p>计算 Input 1 的 attention score<br>计算公式为：<code>Input 1&#39;s query x keys^T</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">            [0, 4, 2]</span><br><span class="line">[1, 0, 2] x [1, 4, 3] = [2, 4, 4]</span><br><span class="line">            [1, 0, 1]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Note: 上述为 dot product attention, 其他 <a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3" target="_blank" rel="noopener">score function</a> 有 scaled dot product 和 additive/concat。</p>
<ul>
<li><p>计算softmax<br>对 attention score 进行 softmax 计算:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax([2, 4, 4]) = [0.0, 0.5, 0.5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>将 score 乘以 value:<br>将 attention score 乘以对应的 value, 获得 weighted values:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]</span><br><span class="line">2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]</span><br><span class="line">3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>将 weighted values 加和获得 Output 1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  [0.0, 0.0, 0.0]</span><br><span class="line">+ [1.0, 4.0, 0.0]</span><br><span class="line">+ [1.0, 3.0, 1.5]</span><br><span class="line">-----------------</span><br><span class="line">= [2.0, 7.0, 1.5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于 Input 2 和 Input 3 重复4-7操作<br>Note: query 和 key 的维度需要保持一致，而 value 的维度与 output 一致。</p>
</li>
</ul>
</div><div class="tags"><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a></div><div class="post-nav"><a class="pre" href="/2019/12/09/pytorch/">Pytorch 基本概念</a><a class="next" href="/2019/11/12/jupyter/">Jupyter使用笔记</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'AMlVKrsPTMdYDOomfWIdGiFC-gzGzoHsz',
  appKey:'Msy2N1GDdBCf0W3MY9fjRYHc',
  placeholder:'Comments',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>当你凝视深渊的时候，深渊也在凝视着你。</p><a class="info-icon" href="https://twitter.com/yren_zj" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure/">Data Structure</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML4Med/">ML4Med</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/EHR/" style="font-size: 15px;">EHR</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/26/med-bert/">Med BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/18/mldl/">Lable Distribution for Multimodal Machine Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/cat/">cat</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/13/keras/">Keras 学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/20/BERT/">BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/26/offer/">剑指offer</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/23/matplotlib/">Matplotlib绘图</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/22/ai4md/">AI for Medicine</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/01/ml/">Machine Learning 面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/30/tensorflow-2-0/">tensorflow-2.x</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Sceliay's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>