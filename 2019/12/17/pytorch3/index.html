<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Pytorch 序列 | Sceliay's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Pytorch 序列</h1><a id="logo" href="/.">Sceliay's Blog</a><p class="description">Welcome to my blog!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Pytorch 序列</h1><div class="post-meta">2019-12-17<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="post-content"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>参考：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/PyTorch-introduction.ipynb">code of learn deep learning with pytroch</a></p>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><script type="math/tex; mode=display">h_t = tanh(w_{ih}*x_t+b_{ih}+w_{hh}*h_{t-1}+b_{hh})</script><ol>
<li><p><code>torch.nn.RNNCell()</code> 只接受序列中单步的输入，且必须传入隐藏状态</p>
<ul>
<li><code>input_size</code>: 输入的特征维度</li>
<li><code>hidden_size</code>: 输出的特征维度</li>
<li><code>num_layers</code>: 网络的层数</li>
<li><code>nonlinearity</code>: 非线性激活函数，默认 ‘tanh’</li>
<li><code>bias</code>: 是否使用偏置，默认使用</li>
<li><code>batch_first</code>: 输入数据的形式，默认 False，为(seq, batch, feature)，</li>
<li><code>dropout</code>: 是否应用 dropout</li>
<li><code>bidirectional</code>: 是否使用双向 rnn，默认 False<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line"># 定义一个单步的 rnn</span><br><span class="line">rnn_single = nn.RNNCell(input_size=100, hidden_size=200)</span><br><span class="line"></span><br><span class="line"># 访问其中的参数</span><br><span class="line">rnn_single.weight_hh</span><br><span class="line"></span><br><span class="line"># 构造一个序列，长为 6，batch 是 5， 特征是 100</span><br><span class="line">x = Variable(torch.randn(6, 5, 100)) # 这是 rnn 的输入格式</span><br><span class="line"></span><br><span class="line"># 定义初始的记忆状态</span><br><span class="line">h_t = Variable(torch.zeros(5, 200))</span><br><span class="line"></span><br><span class="line"># 传入 rnn</span><br><span class="line">out = []</span><br><span class="line">for i in range(6): # 通过循环 6 次作用在整个序列上</span><br><span class="line">    h_t = rnn_single(x[i], h_t)</span><br><span class="line">    out.append(h_t)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>torch.nn.RNN()</code> 可以接受一个序列的输入，默认为全0的隐藏状态，可以自己申明</p>
<ul>
<li><code>input_size</code></li>
<li><code>hidden_size</code></li>
<li><code>bias</code></li>
<li><code>nonlinearity</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rnn_seq = nn.RNN(100, 200)</span><br><span class="line"></span><br><span class="line"># 访问其中的参数</span><br><span class="line">rnn_seq.weight_hh_l0</span><br><span class="line"></span><br><span class="line"># 使用默认的全 0 隐藏状态</span><br><span class="line">out, h_t = rnn_seq(x) </span><br><span class="line"></span><br><span class="line"># 自己定义初始的隐藏状态</span><br><span class="line">h_0 = Variable(torch.randn(1, 5, 200))</span><br><span class="line">out, h_t = rnn_seq(x, h_0)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>输出的结果均为 (seq, batch, feature)</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><script type="math/tex; mode=display">f_t = \sigma (W_f \cdot [h_{t-1},x_t]+b_f) \\
i_t = \sigma (W_i \cdot [h_{t-1},x_t]+b_i) \\
\tilde{C_t} = tanh (W_C \cdot [h_{t-1},x_t]+b_C) \\
C_t = f_t*C_{t-1}+i_t *\tilde{C_t} \\
o_t = \sigma(w_o \cdot [h_{t-1},x_t]+b_o) \\
h_t = o_t*tanh(C_t)</script><p>LSTM 与基本 RNN 一样，参数也相同，具有<code>nn.LSTMCell()</code>和<code>nn.LSTM()</code>两种形式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lstm_seq = nn.LSTM(50, 100, num_layers=2) # 输入维度 50，输出 100，两层</span><br><span class="line"></span><br><span class="line">lstm_seq.weight_hh_l0 # 第一层的 h_t 权重</span><br><span class="line"></span><br><span class="line">lstm_input = Variable(torch.randn(10, 3, 50)) # 序列 10，batch 是 3，输入维度 50</span><br><span class="line"></span><br><span class="line">out, (h, c) = lstm_seq(lstm_input) # 使用默认的全 0 隐藏状态</span><br><span class="line"></span><br><span class="line"># 不使用默认的隐藏状态</span><br><span class="line">h_init = Variable(torch.randn(2, 3, 100))</span><br><span class="line">c_init = Variable(torch.randn(2, 3, 100))</span><br><span class="line"></span><br><span class="line">out, (h, c) = lstm_seq(lstm_input, (h_init, c_init))</span><br></pre></td></tr></table></figure></p>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><script type="math/tex; mode=display">z_t = \sigma (W_z \cdot [h_{t-1},x_t]) \\
r_t = \sigma (W_r \cdot [h_{t-1},x_t]) \\
\tilde{h_t} = tanh(W \cdot [r_t * h_{t-1}, x_t]) \\
h_t = (1-z_t)*h_{t-1}+z_t*\tilde{h_t}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gru_seq = nn.GRU(10, 20)</span><br><span class="line">gru_input = Variable(torch.randn(3, 32, 10))</span><br><span class="line"></span><br><span class="line">out, h = gru_seq(gru_input)</span><br></pre></td></tr></table></figure>
<h2 id="RNN用于时间序列分析"><a href="#RNN用于时间序列分析" class="headerlink" title="RNN用于时间序列分析"></a>RNN用于时间序列分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">data_csv = pd.read_csv(&apos;./data.csv&apos;, usecols=[1])</span><br><span class="line"></span><br><span class="line"># 数据预处理</span><br><span class="line">data_csv = data_csv.dropna()</span><br><span class="line">dataset = data_csv.values</span><br><span class="line">dataset = dataset.astype(&apos;float32&apos;)</span><br><span class="line">max_value = np.max(dataset)</span><br><span class="line">min_value = np.min(dataset)</span><br><span class="line">scalar = max_value - min_value</span><br><span class="line">dataset = list(map(lambda x: (x-min_value) / scalar, dataset))</span><br><span class="line"></span><br><span class="line"># 通过前两个月的流量来预测当月流量</span><br><span class="line">def create_dataset(dataset, look_back=2):</span><br><span class="line">    dataX, dataY = [], []</span><br><span class="line">    for i in range(len(dataset) - look_back):</span><br><span class="line">        a = dataset[i:(i + look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i + look_back])</span><br><span class="line">    return np.array(dataX), np.array(dataY)</span><br><span class="line"></span><br><span class="line"># 创建好输入输出</span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br><span class="line"></span><br><span class="line"># 划分训练集和测试集，70% 作为训练集</span><br><span class="line">train_size = int(len(data_X) * 0.7)</span><br><span class="line">test_size = len(data_X) - train_size</span><br><span class="line">train_X = data_X[:train_size]</span><br><span class="line">train_Y = data_Y[:train_size]</span><br><span class="line">test_X = data_X[train_size:]</span><br><span class="line">test_Y = data_Y[train_size:]</span><br><span class="line"></span><br><span class="line"># RNN 读入数据维度为 (seq,batch,feature)</span><br><span class="line">train_X = train_X.reshape(-1, 1, 2)</span><br><span class="line">train_Y = train_Y.reshape(-1, 1, 1)</span><br><span class="line">test_X = test_X.reshape(-1, 1, 2)</span><br><span class="line"></span><br><span class="line">train_x = torch.from_numpy(train_X)</span><br><span class="line">train_y = torch.from_numpy(train_Y)</span><br><span class="line">test_x = torch.from_numpy(test_X)</span><br><span class="line"></span><br><span class="line"># 定义模型</span><br><span class="line">class lstm_reg(nn.Module):</span><br><span class="line">    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):</span><br><span class="line">        super(lstm_reg, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.LSTM(input_size, hidden_size, num_layers) # rnn</span><br><span class="line">        self.reg = nn.Linear(hidden_size, output_size) # 回归</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x, _ = self.rnn(x) # (seq, batch, hidden)</span><br><span class="line">        s, b, h = x.shape</span><br><span class="line">        x = x.view(s*b, h) # 转换成线性层的输入格式</span><br><span class="line">        x = self.reg(x)</span><br><span class="line">        x = x.view(s, b, -1)</span><br><span class="line">        return x</span><br><span class="line"># 输入维度为2，隐层为4</span><br><span class="line">net = lstm_reg(2, 4)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)</span><br><span class="line"></span><br><span class="line"># 开始训练</span><br><span class="line">for e in range(1000):</span><br><span class="line">    var_x = Variable(train_x)</span><br><span class="line">    var_y = Variable(train_y)</span><br><span class="line">    # 前向传播</span><br><span class="line">    out = net(var_x)</span><br><span class="line">    loss = criterion(out, var_y)</span><br><span class="line">    # 反向传播</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    if (e + 1) % 100 == 0: # 每 100 次输出结果</span><br><span class="line">        print(&apos;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&apos;.format(e + 1, loss.data[0]))</span><br><span class="line"></span><br><span class="line"># 预测结果</span><br><span class="line">net = net.eval() # 转换成测试模式</span><br><span class="line"></span><br><span class="line">data_X = data_X.reshape(-1, 1, 2)</span><br><span class="line">data_X = torch.from_numpy(data_X)</span><br><span class="line">var_data = Variable(data_X)</span><br><span class="line">pred_test = net(var_data) # 测试集的预测结果</span><br><span class="line"></span><br><span class="line"># 改变输出的格式，view=reshape</span><br><span class="line">pred_test = pred_test.view(-1).data.numpy()</span><br><span class="line"></span><br><span class="line"># 画出实际结果和预测的结果</span><br><span class="line">plt.plot(pred_test, &apos;r&apos;, label=&apos;prediction&apos;)</span><br><span class="line">plt.plot(dataset, &apos;b&apos;, label=&apos;real&apos;)</span><br><span class="line">plt.legend(loc=&apos;best&apos;)</span><br></pre></td></tr></table></figure>
<h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># 定义词嵌入</span><br><span class="line">embeds = nn.Embedding(2, 5) # 2 个单词，维度 5</span><br><span class="line"></span><br><span class="line"># 得到词嵌入矩阵</span><br><span class="line">embeds.weight</span><br><span class="line"></span><br><span class="line"># 直接手动修改词嵌入的值</span><br><span class="line">embeds.weight.data = torch.ones(2, 5)</span><br><span class="line"></span><br><span class="line"># 访问第 50 个词的词向量</span><br><span class="line">embeds = nn.Embedding(100, 10)</span><br><span class="line">single_word_embed = embeds(Variable(torch.LongTensor([50])))</span><br></pre></td></tr></table></figure>
<h1 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = 2 # 依据的单词数</span><br><span class="line">EMBEDDING_DIM = 10 # 词向量的维度</span><br><span class="line"># 我们使用莎士比亚的诗</span><br><span class="line">test_sentence = &quot;&quot;&quot;When forty winters shall besiege thy brow,</span><br><span class="line">And dig deep trenches in thy beauty&apos;s field,</span><br><span class="line">Thy youth&apos;s proud livery so gazed on now,</span><br><span class="line">Will be a totter&apos;d weed of small worth held:</span><br><span class="line">Then being asked, where all thy beauty lies,</span><br><span class="line">Where all the treasure of thy lusty days;</span><br><span class="line">To say, within thine own deep sunken eyes,</span><br><span class="line">Were an all-eating shame, and thriftless praise.</span><br><span class="line">How much more praise deserv&apos;d thy beauty&apos;s use,</span><br><span class="line">If thou couldst answer &apos;This fair child of mine</span><br><span class="line">Shall sum my count, and make my old excuse,&apos;</span><br><span class="line">Proving his beauty by succession thine!</span><br><span class="line">This were to be new made when thou art old,</span><br><span class="line">And see thy blood warm when thou feel&apos;st it cold.&quot;&quot;&quot;.split()</span><br><span class="line"></span><br><span class="line"># 创建数据集</span><br><span class="line">trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2]) </span><br><span class="line">            for i in range(len(test_sentence)-2)]</span><br><span class="line"></span><br><span class="line"># 建立每个词与数字的编码，据此构建词嵌入</span><br><span class="line">vocb = set(test_sentence) # 使用 set 将重复的元素去掉</span><br><span class="line">word_to_idx = &#123;word: i for i, word in enumerate(vocb)&#125;</span><br><span class="line">idx_to_word = &#123;word_to_idx[word]: word for word in word_to_idx&#125;</span><br><span class="line"></span><br><span class="line"># 定义模型</span><br><span class="line">class n_gram(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, context_size=CONTEXT_SIZE, n_dim=EMBEDDING_DIM):</span><br><span class="line">        super(n_gram, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.embed = nn.Embedding(vocab_size, n_dim)</span><br><span class="line">        self.classify = nn.Sequential(</span><br><span class="line">            nn.Linear(context_size * n_dim, 128),</span><br><span class="line">            nn.ReLU(True),</span><br><span class="line">            nn.Linear(128, vocab_size)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        voc_embed = self.embed(x) # 得到词嵌入</span><br><span class="line">        voc_embed = voc_embed.view(1, -1) # 将两个词向量拼在一起</span><br><span class="line">        out = self.classify(voc_embed)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">net = n_gram(len(word_to_idx))</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, weight_decay=1e-5)</span><br><span class="line"></span><br><span class="line">for e in range(100):</span><br><span class="line">    train_loss = 0</span><br><span class="line">    for word, label in trigram: # 使用前 100 个作为训练集</span><br><span class="line">        word = Variable(torch.LongTensor([word_to_idx[i] for i in word])) # 将两个词作为输入</span><br><span class="line">        label = Variable(torch.LongTensor([word_to_idx[label]]))</span><br><span class="line">        # 前向传播</span><br><span class="line">        out = net(word)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        train_loss += loss.data</span><br><span class="line">        # 反向传播</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    if (e + 1) % 20 == 0:</span><br><span class="line">        print(&apos;epoch: &#123;&#125;, Loss: &#123;:.6f&#125;&apos;.format(e + 1, train_loss / len(trigram)))</span><br><span class="line"></span><br><span class="line">net = net.eval()</span><br><span class="line"></span><br><span class="line"># 测试一下结果</span><br><span class="line">word, label = trigram[19]</span><br><span class="line">print(&apos;input: &#123;&#125;&apos;.format(word))</span><br><span class="line">print(&apos;label: &#123;&#125;&apos;.format(label))</span><br><span class="line">print()</span><br><span class="line">word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))</span><br><span class="line">out = net(word)</span><br><span class="line"></span><br><span class="line">pred_label_idx = out.max(1)[1].data.numpy()[0]</span><br><span class="line">print(pred_label_idx)</span><br><span class="line">predict_word = idx_to_word[pred_label_idx]</span><br><span class="line">print(&apos;real word is &#123;&#125;, predicted word is &#123;&#125;&apos;.format(label, predict_word))</span><br></pre></td></tr></table></figure>
<h1 id="LSTM-做词性预测"><a href="#LSTM-做词性预测" class="headerlink" title="LSTM 做词性预测"></a>LSTM 做词性预测</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">training_data = [(&quot;The dog ate the apple&quot;.split(),</span><br><span class="line">                  [&quot;DET&quot;, &quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;]),</span><br><span class="line">                 (&quot;Everybody read that book&quot;.split(), </span><br><span class="line">                  [&quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;])]</span><br><span class="line"></span><br><span class="line"># 对单词和标签编码</span><br><span class="line">word_to_idx = &#123;&#125;</span><br><span class="line">tag_to_idx = &#123;&#125;</span><br><span class="line">for context, tag in training_data:</span><br><span class="line">    for word in context:</span><br><span class="line">        if word.lower() not in word_to_idx:</span><br><span class="line">            word_to_idx[word.lower()] = len(word_to_idx)</span><br><span class="line">    for label in tag:</span><br><span class="line">        if label.lower() not in tag_to_idx:</span><br><span class="line">            tag_to_idx[label.lower()] = len(tag_to_idx)</span><br><span class="line"></span><br><span class="line"># 对字母编码</span><br><span class="line">alphabet = &apos;abcdefghijklmnopqrstuvwxyz&apos;</span><br><span class="line">char_to_idx = &#123;&#125;</span><br><span class="line">for i in range(len(alphabet)):</span><br><span class="line">    char_to_idx[alphabet[i]] = i</span><br><span class="line"></span><br><span class="line"># 构建训练数据</span><br><span class="line">def make_sequence(x, dic): # 字符编码</span><br><span class="line">    idx = [dic[i.lower()] for i in x]</span><br><span class="line">    idx = torch.LongTensor(idx)</span><br><span class="line">    return idx</span><br><span class="line"></span><br><span class="line"># 构建单个字符的 lstm 模型</span><br><span class="line">class char_lstm(nn.Module):</span><br><span class="line">    def __init__(self, n_char, char_dim, char_hidden):</span><br><span class="line">        super(char_lstm, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.char_embed = nn.Embedding(n_char, char_dim)</span><br><span class="line">        self.lstm = nn.LSTM(char_dim, char_hidden)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.char_embed(x)</span><br><span class="line">        out, _ = self.lstm(x)</span><br><span class="line">        return out[-1] # (batch, hidden)</span><br><span class="line"></span><br><span class="line"># 构建词性分类的 lstm 模型</span><br><span class="line">class lstm_tagger(nn.Module):</span><br><span class="line">    def __init__(self, n_word, n_char, char_dim, word_dim, </span><br><span class="line">                 char_hidden, word_hidden, n_tag):</span><br><span class="line">        super(lstm_tagger, self).__init__()</span><br><span class="line">        self.word_embed = nn.Embedding(n_word, word_dim)</span><br><span class="line">        self.char_lstm = char_lstm(n_char, char_dim, char_hidden)</span><br><span class="line">        self.word_lstm = nn.LSTM(word_dim + char_hidden, word_hidden)</span><br><span class="line">        self.classify = nn.Linear(word_hidden, n_tag)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x, word):</span><br><span class="line">        char = []</span><br><span class="line">        for w in word: # 对于每个单词做字符的 lstm</span><br><span class="line">            char_list = make_sequence(w, char_to_idx)</span><br><span class="line">            char_list = char_list.unsqueeze(1) # (seq, batch, feature) 满足 lstm 输入条件</span><br><span class="line">            char_infor = self.char_lstm(Variable(char_list)) # (batch, char_hidden)</span><br><span class="line">            char.append(char_infor)</span><br><span class="line">        char = torch.stack(char, dim=0) # (seq, batch, feature)</span><br><span class="line">        </span><br><span class="line">        x = self.word_embed(x) # (batch, seq, word_dim)</span><br><span class="line">        x = x.permute(1, 0, 2) # 改变顺序</span><br><span class="line">        x = torch.cat((x, char), dim=2) # 沿着特征通道将每个词的词嵌入和字符 lstm 输出的结果拼接在一起</span><br><span class="line">        x, _ = self.word_lstm(x)</span><br><span class="line">        </span><br><span class="line">        s, b, h = x.shape</span><br><span class="line">        x = x.view(-1, h) # 重新 reshape 进行分类线性层</span><br><span class="line">        out = self.classify(x)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">net = lstm_tagger(len(word_to_idx), len(char_to_idx), 10, 100, 50, 128, len(tag_to_idx))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)</span><br><span class="line"></span><br><span class="line"># 开始训练</span><br><span class="line">for e in range(300):</span><br><span class="line">    train_loss = 0</span><br><span class="line">    for word, tag in training_data:</span><br><span class="line">        word_list = make_sequence(word, word_to_idx).unsqueeze(0) # 添加第一维 batch</span><br><span class="line">        tag = make_sequence(tag, tag_to_idx)</span><br><span class="line">        word_list = Variable(word_list)</span><br><span class="line">        tag = Variable(tag)</span><br><span class="line">        # 前向传播</span><br><span class="line">        out = net(word_list, word)</span><br><span class="line">        loss = criterion(out, tag)</span><br><span class="line">        train_loss += loss.data[0]</span><br><span class="line">        # 反向传播</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    if (e + 1) % 50 == 0:</span><br><span class="line">        print(&apos;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&apos;.format(e + 1, train_loss / len(training_data)))</span><br><span class="line"></span><br><span class="line"># 预测</span><br><span class="line">net = net.eval()</span><br><span class="line"></span><br><span class="line">test_sent = &apos;Everybody ate the apple&apos;</span><br><span class="line">test = make_sequence(test_sent.split(), word_to_idx).unsqueeze(0)</span><br><span class="line">out = net(Variable(test), test_sent.split())</span><br><span class="line"></span><br><span class="line">print(tag_to_idx)</span><br></pre></td></tr></table></figure></div><div class="tags"><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a><a href="/tags/Python"><i class="fa fa-tag">Python</i></a><a href="/tags/Pytorch"><i class="fa fa-tag">Pytorch</i></a></div><div class="post-nav"><a class="pre" href="/2020/01/06/anaconda/">服务器环境命令</a><a class="next" href="/2019/12/17/pytorch2/">Pytorch 模型</a></div><div id="container"></div><link rel="stylesheet" type="text/css" href="//unpkg.com/gitalk/dist/gitalk.css"><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script><script type="text/javascript" src="//unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
  clientID: '179d25b3988d41a8a145',
  clientSecret: '7812569c157b17fe879957b54b477810952b77c1',
  repo: 'sceliay.github.io/',
  owner: 'sceliay',
  admin: ['sceliay'],
  id: md5(location.pathname),
  distractionFreeMode: false
})
gitalk.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>当你凝视深渊的时候，深渊也在凝视着你。</p><a class="info-icon" href="https://twitter.com/yren_zj" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure/">Data Structure</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/EHR/" style="font-size: 15px;">EHR</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/Notes/" style="font-size: 15px;">Notes</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/26/med-bert/">Med BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/18/mldl/">MLDL</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/cat/">cat</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/13/keras-1/">Keras 学习笔记(2)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/20/BERT/">BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/28/ai4md2/">AI for Medicine(2)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/26/offer/">剑指offer</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/23/matplotlib/">Matplotlib</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/22/ai4md/">AI for Medicine</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/01/ml/">Machine Learning</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Sceliay's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>