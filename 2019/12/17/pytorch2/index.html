<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Pytorch 模型 | Sceliay's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Pytorch 模型</h1><a id="logo" href="/.">Sceliay's Blog</a><p class="description">Welcome to my blog!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Pytorch 模型</h1><div class="post-meta">2019-12-17<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><a class="disqus-comment-count" href="/2019/12/17/pytorch2/#vcomment"><span class="valine-comment-count" data-xid="/2019/12/17/pytorch2/"></span><span> 条评论</span></a><div class="post-content"><p>参考：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/PyTorch-introduction.ipynb">code of learn deep learning with pytroch</a></p>
<h1 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h1><p>Sequential 构建序列化模块。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_net = nn.Sequential(</span><br><span class="line">	nn.Linear(2,4),</span><br><span class="line">	nn.Tanh(),</span><br><span class="line">	nn.Linear(4,1)</span><br><span class="line">	)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>序列模块可以通过索引访问每一层</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seq_net[0] # 第一层</span><br><span class="line">[out]: Linear(in_features=2, out_features=4)</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印权重</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w0 = seq_net[0].weight</span><br><span class="line">print(w0)</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 通过 parameters 可以取得模型的参数</span><br><span class="line">param = seq_net.parameters()</span><br><span class="line"></span><br><span class="line"># 定义优化器</span><br><span class="line">optim = torch.optim.SGD(param, 1.)</span><br><span class="line"></span><br><span class="line"># 训练 10000 次</span><br><span class="line">for e in range(10000):</span><br><span class="line">    out = seq_net(Variable(x))</span><br><span class="line">    loss = criterion(out, Variable(y))</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    if (e + 1) % 1000 == 0:</span><br><span class="line">        print(&apos;epoch: &#123;&#125;, loss: &#123;&#125;&apos;.format(e+1, loss.data))</span><br></pre></td></tr></table></figure>
</li>
<li><p>保存参数和模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 将参数和模型保存在一起</span><br><span class="line">torch.save(seq_net, &apos;save_seq_net.pth&apos;)</span><br><span class="line"></span><br><span class="line"># 读取保存的模型</span><br><span class="line">seq_net1 = torch.load(&apos;save_seq_net.pth&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>保存参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 保存模型参数</span><br><span class="line">torch.save(seq_net.state_dict(), &apos;save_seq_net_params.pth&apos;)</span><br><span class="line"></span><br><span class="line"># 先定义模型，再读取参数</span><br><span class="line">seq_net2 = nn.Sequential(</span><br><span class="line">    nn.Linear(2, 4),</span><br><span class="line">    nn.Tanh(),</span><br><span class="line">    nn.Linear(4, 1)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">seq_net2.load_state_dict(torch.load(&apos;save_seq_net_params.pth&apos;))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h1><p>更加灵活的模型定义方式。使用 Module 的模板：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class 网络名字(nn.Module):</span><br><span class="line">    def __init__(self, 一些定义的参数):</span><br><span class="line">        super(网络名字, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(num_input, num_hidden)</span><br><span class="line">        self.layer2 = nn.Sequential(...)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        定义需要用的网络层</span><br><span class="line"></span><br><span class="line">    def forward(self, x): # 定义前向传播</span><br><span class="line">        x1 = self.layer1(x)</span><br><span class="line">        x2 = self.layer2(x)</span><br><span class="line">        x = x1 + x2</span><br><span class="line">        ...</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>实现上述神经网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class module_net(nn.Module):</span><br><span class="line">    def __init__(self, num_input, num_hidden, num_output):</span><br><span class="line">        super(module_net, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(num_input, num_hidden)</span><br><span class="line">        </span><br><span class="line">        self.layer2 = nn.Tanh()</span><br><span class="line">        </span><br><span class="line">        self.layer3 = nn.Linear(num_hidden, num_output)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mo_net = module_net(2, 4, 1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问模型中的某层可以直接通过名字</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 第一层</span><br><span class="line">l1 = mo_net.layer1</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印权值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(l1.weight)</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 定义优化器</span><br><span class="line">optim = torch.optim.SGD(mo_net.parameters(), 1.)</span><br><span class="line"></span><br><span class="line"># 训练1000次</span><br><span class="line">for e in range(10000):</span><br><span class="line">    out = mo_net(Variable(x))</span><br><span class="line">    loss = criterion(out, Variable(y))</span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line">    if (e + 1) % 1000 == 0:</span><br><span class="line">        print(&apos;epoch: &#123;&#125;, loss: &#123;&#125;&apos;.format(e+1, loss.data[0]))</span><br><span class="line"></span><br><span class="line"># 保存模型</span><br><span class="line">torch.save(mo_net.state_dict(), &apos;module_net.pth&apos;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载 MNIST 数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def data_tf(x):</span><br><span class="line">    x = np.array(x, dtype=&apos;float32&apos;) / 255</span><br><span class="line">    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到</span><br><span class="line">    x = x.reshape((-1,)) # 拉平</span><br><span class="line">    x = torch.from_numpy(x)</span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line">train_set = mnist.MNIST(&apos;./data&apos;, train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换</span><br><span class="line">test_set = mnist.MNIST(&apos;./data&apos;, train=False, transform=data_tf, download=True)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 DataLoader 定义一个数据迭代器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">train_data = DataLoader(train_set, batch_size=64, shuffle=True)</span><br><span class="line">test_data = DataLoader(test_set, batch_size=128, shuffle=False)</span><br><span class="line"></span><br><span class="line"># iter()获取迭代对象的迭代器，next()获取下一条数据</span><br><span class="line">a, a_label = next(iter(train_data))</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制图像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.title(&apos;train loss&apos;)</span><br><span class="line">plt.plot(np.arange(len(losses)), losses)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h1><ul>
<li><p>可以通过<code>.weigth</code>和<code>.bias</code>访问网络的权值，并通过<code>.data</code>访问其数值，并替换：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个 Sequential 模型</span><br><span class="line">net1 = nn.Sequential(</span><br><span class="line">    nn.Linear(30, 40),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(40, 50),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(50, 10)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 定义一个 Tensor 直接对其进行替换</span><br><span class="line">net1[0].weight.data = torch.from_numpy(np.random.uniform(3, 5, size=(40, 30)))</span><br><span class="line"></span><br><span class="line"># 模型中相同类型的层需要相同初始化方式</span><br><span class="line">for layer in net1:</span><br><span class="line">    if isinstance(layer, nn.Linear): # 判断是否是线性层</span><br><span class="line">        param_shape = layer.weight.shape</span><br><span class="line">        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape)) </span><br><span class="line">        # 定义为均值为 0，方差为 0.5 的正态分布</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于 Module 的参数化，可以直接像 Sequential 一样对其 Tensor 进行重新定义。如果用循环方式，需要介绍两个属性，children 和 modules, children 只访问到模型定义中的第一层，modules 会访问到最后的结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">class sim_net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(sim_net, self).__init__()</span><br><span class="line">        self.l1 = nn.Sequential(</span><br><span class="line">            nn.Linear(30, 40),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.l1[0].weight.data = torch.randn(40, 30) # 直接对某一层初始化</span><br><span class="line">        </span><br><span class="line">        self.l2 = nn.Sequential(</span><br><span class="line">            nn.Linear(40, 50),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.l3 = nn.Sequential(</span><br><span class="line">            nn.Linear(50, 10),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.l1(x)</span><br><span class="line">        x =self.l2(x)</span><br><span class="line">        x = self.l3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net2 = sim_net()</span><br><span class="line"></span><br><span class="line"># 访问 children</span><br><span class="line">for i in net2.children():</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line"># 访问 modules</span><br><span class="line">for i in net2.modules():</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line"># 迭代初始化</span><br><span class="line">for layer in net2.modules():</span><br><span class="line">    if isinstance(layer, nn.Linear):</span><br><span class="line">        param_shape = layer.weight.shape</span><br><span class="line">        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape))</span><br></pre></td></tr></table></figure>
</li>
<li><p>torch.nn.init</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torch.nn import init</span><br><span class="line"></span><br><span class="line">init.xavier_uniform(net1[0].weight)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><ul>
<li><p>随机梯度下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def sgd_update(parameters, lr):</span><br><span class="line">    for param in parameters:</span><br><span class="line">        param.data = param.data - lr * param.grad.data</span><br><span class="line"></span><br><span class="line">optimzier = torch.optim.SGD(net.parameters(), 1e-2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>动量法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def sgd_momentum(parameters, vs, lr, gamma):</span><br><span class="line">    for param, v in zip(parameters, vs):</span><br><span class="line">        v[:] = gamma * v + lr * param.grad.data</span><br><span class="line">        param.data = param.data - v</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # 加动量</span><br></pre></td></tr></table></figure>
</li>
<li><p>Adagrad</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def sgd_adagrad(parameters, sqrs, lr):</span><br><span class="line">    eps = 1e-10</span><br><span class="line">    for param, sqr in zip(parameters, sqrs):</span><br><span class="line">        sqr[:] = sqr + param.grad.data ** 2</span><br><span class="line">        div = lr / torch.sqrt(sqr + eps) * param.grad.data</span><br><span class="line">        param.data = param.data - div</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>RMSProp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def rmsprop(parameters, sqrs, lr, alpha):</span><br><span class="line">    eps = 1e-10</span><br><span class="line">    for param, sqr in zip(parameters, sqrs):</span><br><span class="line">        sqr[:] = alpha * sqr + (1 - alpha) * param.grad.data ** 2</span><br><span class="line">        div = lr / torch.sqrt(sqr + eps) * param.grad.data</span><br><span class="line">        param.data = param.data - div</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, alpha=0.9)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Adadelta</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def adadelta(parameters, sqrs, deltas, rho):</span><br><span class="line">    eps = 1e-6</span><br><span class="line">    for param, sqr, delta in zip(parameters, sqrs, deltas):</span><br><span class="line">        sqr[:] = rho * sqr + (1 - rho) * param.grad.data ** 2</span><br><span class="line">        cur_delta = torch.sqrt(delta + eps) / torch.sqrt(sqr + eps) * param.grad.data</span><br><span class="line">        delta[:] = rho * delta + (1 - rho) * cur_delta ** 2</span><br><span class="line">        param.data = param.data - cur_delta</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adadelta(net.parameters(), rho=0.9)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Adam</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def adam(parameters, vs, sqrs, lr, t, beta1=0.9, beta2=0.999):</span><br><span class="line">    eps = 1e-8</span><br><span class="line">    for param, v, sqr in zip(parameters, vs, sqrs):</span><br><span class="line">        v[:] = beta1 * v + (1 - beta1) * param.grad.data</span><br><span class="line">        sqr[:] = beta2 * sqr + (1 - beta2) * param.grad.data ** 2</span><br><span class="line">        v_hat = v / (1 - beta1 ** t)</span><br><span class="line">        s_hat = sqr / (1 - beta2 ** t)</span><br><span class="line">        param.data = param.data - lr * v_hat / torch.sqrt(s_hat + eps)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</div><div class="tags"><a href="/tags/Python"><i class="fa fa-tag">Python</i></a><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a><a href="/tags/Pytorch"><i class="fa fa-tag">Pytorch</i></a></div><div class="post-nav"><a class="pre" href="/2019/12/17/pytorch3/">Pytorch 序列</a><a class="next" href="/2019/12/09/pytorch/">Pytorch 基本概念</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'AMlVKrsPTMdYDOomfWIdGiFC-gzGzoHsz',
  appKey:'Msy2N1GDdBCf0W3MY9fjRYHc',
  placeholder:'Comments',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>当你凝视深渊的时候，深渊也在凝视着你。</p><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://weibo.com/u/2265958580" title="weibo" target="_blank" style="margin-inline:5px"> <i class="fa fa-weibo" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure/">Data Structure</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML4Med/">ML4Med</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/EHR/" style="font-size: 15px;">EHR</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/12/02/BERT/">BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/10/11/tensorflow-1/">Tensorflow模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/26/med-bert/">Med BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/18/mldl/">Lable Distribution for Multimodal Machine Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/cat/">大黄饲养笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/13/keras/">Keras 学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/26/offer/">剑指offer</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/23/matplotlib/">Matplotlib绘图</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/22/ai4md/">AI for Medicine</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/01/ml/">Machine Learning 面试题</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Sceliay's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>