<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Tensorflow 2.0 | YRen's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Tensorflow 2.0</h1><a id="logo" href="/.">YRen's Blog</a><p class="description">当你凝视深渊的时候，深渊也在凝视着你。</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Tensorflow 2.0</h1><div class="post-meta">2020-03-30<span> | </span><span class="category"><a href="/categories/Tensorflow/">Tensorflow</a></span></div><a class="disqus-comment-count" href="/2020/03/30/tensorflow-2-0/#vcomment"><span class="valine-comment-count" data-xid="/2020/03/30/tensorflow-2-0/"></span><span> 条评论</span></a><div class="post-content"><p>Tensorflow 2.x的学习笔记。主要教程为<a href="https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book">Deep-Learning-with-TensorFlow-book</a>，<a href="https://hecongqing.gitbook.io/tfnotes/" target="_blank" rel="noopener">tensorflow 2.0 实战笔记</a></p>
<h2 id="Tensorflow-2-x-vs-1-x"><a href="#Tensorflow-2-x-vs-1-x" class="headerlink" title="Tensorflow 2.x vs. 1.x"></a>Tensorflow 2.x vs. 1.x</h2><ul>
<li><p>Tensorflow 1.x 先创建计算图，然后运行，为静态图模式，该编程方法叫做符号式编程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 1.创建计算图阶段</span><br><span class="line"># 创建2个输入端子，指定类型和名字</span><br><span class="line">a_ph = tf.placeholder(tf.float32, name=&apos;variable_a&apos;)</span><br><span class="line">b_ph = tf.placeholder(tf.float32, name=&apos;variable_b&apos;)</span><br><span class="line"># 创建输出端子的运算操作，并命名</span><br><span class="line">c_op = tf.add(a_ph, b_ph, name=&apos;variable_c&apos;)</span><br><span class="line"></span><br><span class="line"># 2.运行计算图阶段</span><br><span class="line"># 创建运行环境</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"># 初始化步骤也需要作为操作运行</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init) # 运行初始化操作，完成初始化</span><br><span class="line"># 运行输出端子，需要给输入端子赋值</span><br><span class="line">c_numpy = sess.run(c_op, feed_dict=&#123;a_ph: 2., b_ph: 4.&#125;)</span><br><span class="line"># 运算完输出端子才能得到数值类型的c_numpy</span><br><span class="line">print(&apos;a+b=&apos;,c_numpy)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Tensorflow 2.x 支持动态图优先模式，即为命令式编程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 1.创建输入张量</span><br><span class="line">a = tf.constant(2.)</span><br><span class="line">b = tf.constant(4.)</span><br><span class="line"># 2.直接计算并打印</span><br><span class="line">print(&apos;a+b=&apos;,a+b)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><ul>
<li><p>加速计算（GPU vs. CPU)</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 创建在CPU 上运算的2个矩阵</span><br><span class="line">with tf.device(&apos;/cpu:0&apos;):</span><br><span class="line">	cpu_a = tf.random.normal([1, n])</span><br><span class="line">	cpu_b = tf.random.normal([n, 1])</span><br><span class="line">	print(cpu_a.device, cpu_b.device)</span><br><span class="line"># 创建使用GPU 运算的2 个矩阵</span><br><span class="line">with tf.device(&apos;/gpu:0&apos;):</span><br><span class="line">	gpu_a = tf.random.normal([1, n])</span><br><span class="line">	gpu_b = tf.random.normal([n, 1])</span><br><span class="line">	print(gpu_a.device, gpu_b.device)</span><br><span class="line"></span><br><span class="line">def cpu_run():</span><br><span class="line">	with tf.device(&apos;/cpu:0&apos;):</span><br><span class="line">		c = tf.matmul(cpu_a, cpu_b)</span><br><span class="line">	return c</span><br><span class="line">def gpu_run():</span><br><span class="line">	with tf.device(&apos;/gpu:0&apos;):</span><br><span class="line">		c = tf.matmul(gpu_a, gpu_b)</span><br><span class="line">	return c</span><br><span class="line"></span><br><span class="line"># 第一次计算需要热身，避免将初始化阶段时间结算在内</span><br><span class="line">cpu_time = timeit.timeit(cpu_run, number=10)</span><br><span class="line">gpu_time = timeit.timeit(gpu_run, number=10)</span><br><span class="line">print(&apos;warmup:&apos;, cpu_time, gpu_time)</span><br><span class="line"># 正式计算10 次，取平均时间</span><br><span class="line">cpu_time = timeit.timeit(cpu_run, number=10)</span><br><span class="line">gpu_time = timeit.timeit(gpu_run, number=10)</span><br><span class="line">print(&apos;run time:&apos;, cpu_time, gpu_time)</span><br></pre></td></tr></table></figure>
<p>  当 <script type="math/tex">n>10^4</script> 后，CPU计算速度明显上升。</p>
</li>
<li><p>自动梯度</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># 创建4 个张量</span><br><span class="line">a = tf.constant(1.)</span><br><span class="line">b = tf.constant(2.)</span><br><span class="line">c = tf.constant(3.)</span><br><span class="line">w = tf.constant(4.)</span><br><span class="line">with tf.GradientTape() as tape:# 构建梯度环境</span><br><span class="line">	tape.watch([w]) # 将w 加入梯度跟踪列表</span><br><span class="line">	# 构建计算过程</span><br><span class="line">	y = a * w**2 + b * w + c</span><br><span class="line"># 求导</span><br><span class="line">[dy_dw] = tape.gradient(y, [w])</span><br><span class="line">print(dy_dw) # 打印出导数</span><br><span class="line"># tf.Tensor(10.0, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用神经网络接口</p>
</li>
</ul>
<h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><ul>
<li>神经元线性模型：<script type="math/tex; mode=display">y=wx+b</script></li>
<li>平方和误差：<script type="math/tex; mode=display">L = \frac{1}{n} \sum_{i=1}^{n}(wx^{(i)}+b-y^{(i)})^2</script></li>
<li>最优参数：<script type="math/tex; mode=display">w^*, b^* = \mathop{\arg\min}_{w,b} \frac{1}{n} \sum_{i=1}^{n}(wx^{(i)}+b-y^{(i)})^2</script></li>
<li>梯度下降法：迭代更新<script type="math/tex; mode=display">w' = w - \eta \frac{\partial L}{\partial w}</script><script type="math/tex; mode=display">b' = w- \eta \frac{\partial L}{\partial b}</script></li>
<li><p>实例：</p>
<ul>
<li><p>采集数据： <script type="math/tex">y=1.477x+0.089+\epsilon, \epsilon \sim \mathbb{N}(0,0.01)</script></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = []# 保存样本集的列表</span><br><span class="line">for i in range(100): # 循环采样100 个点</span><br><span class="line">	x = np.random.uniform(-10., 10.) # 随机采样输入x</span><br><span class="line">	# 采样高斯噪声</span><br><span class="line">	eps = np.random.normal(0., 0.1)</span><br><span class="line">	# 得到模型的输出</span><br><span class="line">	y = 1.477 * x + 0.089 + eps</span><br><span class="line">	data.append([x, y]) # 保存样本点</span><br><span class="line">data = np.array(data) # 转换为2D Numpy 数组</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算误差</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def mse(b, w, points):</span><br><span class="line">	# 根据当前的w,b 参数计算均方差损失</span><br><span class="line">	totalError = 0</span><br><span class="line">	for i in range(0, len(points)): # 循环迭代所有点</span><br><span class="line">		x = points[i, 0] # 获得i 号点的输入x</span><br><span class="line">		y = points[i, 1] # 获得i 号点的输出y</span><br><span class="line">		# 计算差的平方，并累加</span><br><span class="line">		totalError += (y - (w * x + b)) ** 2</span><br><span class="line">	# 将累加的误差求平均，得到均方差</span><br><span class="line">	return totalError / float(len(points))</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算梯度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def step_gradient(b_current, w_current, points, lr):</span><br><span class="line">	# 计算误差函数在所有点上的导数，并更新w,b</span><br><span class="line">	b_gradient = 0</span><br><span class="line">	w_gradient = 0</span><br><span class="line">	M = float(len(points)) # 总样本数</span><br><span class="line">	for i in range(0, len(points)):</span><br><span class="line">		x = points[i, 0]</span><br><span class="line">		y = points[i, 1]</span><br><span class="line">		# 误差函数对b 的导数：grad_b = 2(wx+b-y)，参考公式(2.3)</span><br><span class="line">		b_gradient += (2/M) * ((w_current * x + b_current) - y)</span><br><span class="line">		# 误差函数对w 的导数：grad_w = 2(wx+b-y)*x，参考公式(2.2)</span><br><span class="line">		w_gradient += (2/M) * x * ((w_current * x + b_current) - y)</span><br><span class="line">	# 根据梯度下降算法更新 w&apos;,b&apos;,其中lr 为学习率</span><br><span class="line">	new_b = b_current - (lr * b_gradient)</span><br><span class="line">	new_w = w_current - (lr * w_gradient)</span><br><span class="line">	return [new_b, new_w]</span><br></pre></td></tr></table></figure>
</li>
<li><p>梯度更新</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def gradient_descent(points, starting_b, starting_w, lr, num_iterations):</span><br><span class="line">	# 循环更新w,b 多次</span><br><span class="line">	b = starting_b # b 的初始值</span><br><span class="line">	w = starting_w # w 的初始值</span><br><span class="line">	# 根据梯度下降算法更新多次</span><br><span class="line">	for step in range(num_iterations):</span><br><span class="line">		# 计算梯度并更新一次</span><br><span class="line">		b, w = step_gradient(b, w, np.array(points), lr)</span><br><span class="line">		loss = mse(b, w, points) # 计算当前的均方差，用于监控训练进度</span><br><span class="line">		if step%50 == 0: # 打印误差和实时的w,b 值</span><br><span class="line">			print(f&quot;iteration:&#123;step&#125;, loss:&#123;loss&#125;, w:&#123;w&#125;, b:&#123;b&#125;&quot;)</span><br><span class="line">	return [b, w] # 返回最后一次的w,b</span><br></pre></td></tr></table></figure>
</li>
<li><p>主函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">	# 加载训练集数据，这些数据是通过真实模型添加观测误差采样得到的</span><br><span class="line">	lr = 0.01 # 学习率</span><br><span class="line">	initial_b = 0 # 初始化b 为0</span><br><span class="line">	initial_w = 0 # 初始化w 为0</span><br><span class="line">	num_iterations = 1000</span><br><span class="line">	# 训练优化1000 次，返回最优w*,b*和训练Loss 的下降过程</span><br><span class="line">	[b, w], losses = gradient_descent(data, initial_b, initial_w, lr, num_it</span><br><span class="line">	erations)</span><br><span class="line">	loss = mse(b, w, data) # 计算最优数值解w,b 上的均方差</span><br><span class="line">	print(f&apos;Final loss:&#123;loss&#125;, w:&#123;w&#125;, b:&#123;b&#125;&apos;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><ul>
<li>非线性模型<br>激活函数： sigmoid, relu<script type="math/tex; mode=display">o = \sigma(wx+b)</script></li>
<li><p>下载MNIST数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import tensorflow as tf # 导入TF 库</span><br><span class="line">from tensorflow import keras # 导入TF 子库</span><br><span class="line">from tensorflow.keras import layers, optimizers, datasets # 导入TF 子库</span><br><span class="line"></span><br><span class="line">(x, y), (x_val, y_val) = datasets.mnist.load_data() # 加载数据集</span><br><span class="line">x = 2*tf.convert_to_tensor(x, dtype=tf.float32)/255.-1 # 转换为张量，缩放到-1~1</span><br><span class="line">y = tf.convert_to_tensor(y, dtype=tf.int32) # 转换为张量</span><br><span class="line">y = tf.one_hot(y, depth=10) # one-hot 编码</span><br><span class="line">print(x.shape, y.shape)</span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) # 构建数据集对象</span><br><span class="line">train_dataset = train_dataset.batch(512) # 批量训练</span><br></pre></td></tr></table></figure>
</li>
<li><p>网络搭建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([ # 3 个非线性层的嵌套模型</span><br><span class="line">	layers.Dense(256, activation=&apos;relu&apos;),</span><br><span class="line">	layers.Dense(128, activation=&apos;relu&apos;),</span><br><span class="line">	layers.Dense(10)])</span><br></pre></td></tr></table></figure>
</li>
<li><p>模型训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">with tf.GradientTape() as tape: # 构建梯度记录环境</span><br><span class="line">	# 打平，[b, 28, 28] =&gt; [b, 784]</span><br><span class="line">	x = tf.reshape(x, (-1, 28*28))</span><br><span class="line">	# Step1. 得到模型输出output</span><br><span class="line">	# [b, 784] =&gt; [b, 10]</span><br><span class="line">	out = model(x)</span><br><span class="line">	# step2, 计算mse损失</span><br><span class="line">	loss = mse(out,y)</span><br><span class="line">	# Step3. 计算参数的梯度 w1, w2, w3, b1, b2, b3</span><br><span class="line">	grads = tape.gradient(loss, model.trainable_variables)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</div><div class="tags"><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a><a href="/tags/Tensorflow"><i class="fa fa-tag">Tensorflow</i></a></div><div class="post-nav"><a class="pre" href="/2020/04/01/ml/">Machine Learning 面试题</a><a class="next" href="/2020/01/16/sklearn/">sklearn</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'AMlVKrsPTMdYDOomfWIdGiFC-gzGzoHsz',
  appKey:'Msy2N1GDdBCf0W3MY9fjRYHc',
  placeholder:'Comments',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>YRen</p><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://twitter.com/yren_zj" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://weibo.com/u/2265958580" title="weibo" target="_blank" style="margin-inline:5px"> <i class="fa fa-weibo" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI-for-Medicine/">AI for Medicine</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Pytorch/">Pytorch</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Medicine/" style="font-size: 15px;">Medicine</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/04/06/emergency-ML/">ML for Emergency</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/22/acute/">急危重症预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/09/heterogeneousHealthcare/">Advances in Mining Heterogeneous Healthcare Data</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/02/28/association/">Association mining</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/02/17/transRL/">迁移强化学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/02/16/mimic/">MIMIC 数据集</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/02/03/emergency/">医疗保障急救技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/12/sepsis/">脓毒症</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/09/treatment-medicines-prediction/">Prediction of Treatment Medicines</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/15/label-enhancement/">Label Enhancement</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">YRen's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>