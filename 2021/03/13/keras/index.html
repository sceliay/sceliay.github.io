<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Keras 学习笔记 | Sceliay's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Keras 学习笔记</h1><a id="logo" href="/.">Sceliay's Blog</a><p class="description">Welcome to my blog!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 主页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Keras 学习笔记</h1><div class="post-meta">2021-03-13<span> | </span><span class="category"><a href="/categories/Machine-Learning/">Machine Learning</a></span></div><a class="disqus-comment-count" href="/2021/03/13/keras/#vcomment"><span class="valine-comment-count" data-xid="/2021/03/13/keras/"></span><span> 条评论</span></a><div class="post-content"><p>参考资料：<a href="https://keras.io/zh/" target="_blank" rel="noopener">Keras中文文档</a>,<a href="https://keras.io/" target="_blank" rel="noopener">Keras英文文档</a>, <a href="https://blog.csdn.net/u014061630/article/details/81086564" target="_blank" rel="noopener">Keras教程</a>, <a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-5dym2s6j.html" target="_blank" rel="noopener">W3CSCHOOL</a>,<a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener">Tensorflow中文社区</a>,</p>
<ol>
<li><code>Sequential</code>顺序模型</li>
</ol>
<ul>
<li><p>定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">model = Sequential()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>add</code>堆叠模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Dense</span><br><span class="line">model.add(Dense(units=64, activation=&apos;relu&apos;, input_dim=100))</span><br><span class="line">model.add(Dense(units=10, activation=&apos;softmax&apos;))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>activation</code>: 激活函数</li>
<li><code>kernel_initializer</code>和<code>bias_initializer</code>: 层创建时，权值和偏差的初始化方法，默认为<code>Glorot uniform</code></li>
<li><code>kernel_regularizer</code>和<code>bias_regularizer</code>：层的权重、偏差的正则化方法。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:</span><br><span class="line">layers.Dense(64, kernel_regularizer=keras.regularizers.l1(0.01))</span><br><span class="line"># A linear layer with L2 regularization of factor 0.01 applied to the bias vector:</span><br><span class="line">layers.Dense(64, bias_regularizer=keras.regularizers.l2(0.01))</span><br><span class="line"></span><br><span class="line"># A linear layer with a kernel initialized to a random orthogonal matrix:</span><br><span class="line">layers.Dense(64, kernel_initializer=&apos;orthogonal&apos;)</span><br><span class="line"># A linear layer with a bias vector initialized to 2.0s:</span><br><span class="line">layers.Dense(64, bias_initializer=keras.initializers.constant(2.0))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>使用<code>compile</code>配置学习过程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=&apos;categorical_crossentropy&apos;,</span><br><span class="line">              optimizer=&apos;sgd&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置优化器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=keras.losses.categorical_crossentropy,</span><br><span class="line">              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># x_train 和 y_train 是 Numpy 数组 -- 就像在 Scikit-Learn API 中一样。</span><br><span class="line">model.fit(x_train, y_train, epochs=5, batch_size=32)</span><br></pre></td></tr></table></figure>
</li>
<li><p>评估模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)</span><br></pre></td></tr></table></figure>
</li>
<li><p>预测</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classes = model.predict(x_test, batch_size=128)</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://keras.io/getting-started/sequential-model-guide/" target="_blank" rel="noopener">更多实例</a></p>
</li>
</ul>
<ol>
<li><code>Functional API</code>函数式API</li>
</ol>
<ul>
<li>可用于定义更复杂的模型。</li>
<li>层可调用，返回值为一个tensor</li>
<li>输入tensors和输出tensors被用来定义一个<code>tf.keras.model</code>实例</li>
<li>训练方法与sequential一样</li>
<li><p>eg.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(32,))  # Returns a placeholder tensor</span><br><span class="line"></span><br><span class="line"># A layer instance is callable on a tensor, and returns a tensor.</span><br><span class="line">x = keras.layers.Dense(64, activation=&apos;relu&apos;)(inputs)</span><br><span class="line">x = keras.layers.Dense(64, activation=&apos;relu&apos;)(x)</span><br><span class="line">predictions = keras.layers.Dense(10, activation=&apos;softmax&apos;)(x)</span><br><span class="line"></span><br><span class="line"># Instantiate the model given inputs and outputs.</span><br><span class="line">model = keras.Model(inputs=inputs, outputs=predictions)</span><br><span class="line"></span><br><span class="line"># The compile step specifies the training configuration.</span><br><span class="line">model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),</span><br><span class="line">              loss=&apos;categorical_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line"></span><br><span class="line"># Trains for 5 epochs</span><br><span class="line">model.fit(data, labels, batch_size=32, epochs=5)</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://keras.io/zh/getting-started/functional-api-guide/" target="_blank" rel="noopener">更多实例</a></p>
</li>
</ul>
<ol>
<li>模型的方法和属性</li>
</ol>
<ul>
<li><code>model.layers</code> 是包含模型网络层的展平列表。</li>
<li><code>model.inputs</code> 是模型输入张量的列表。</li>
<li><code>model.outputs</code> 是模型输出张量的列表。</li>
<li><code>model.summary()</code> 打印出模型概述信息。 它是 utils.print_summary 的简捷调用。</li>
<li><code>model.get_config()</code> 返回包含模型配置信息的字典。通过以下代码，就可以根据这些配置信息重新实例化模型：</li>
<li><code>model.get_weights()</code> 返回模型中所有权重张量的列表，类型为 Numpy 数组。</li>
<li><code>model.set_weights(weights)</code> 从 Numpy 数组中为模型设置权重。列表中的数组必须与<code>get_weights()</code>返回的权重具有相同的尺寸。</li>
<li><code>model.save_weights(filepath)</code> 将模型权重存储为 HDF5 文件。</li>
<li><code>model.load_weights(filepath, by_name=False)</code>: 从 HDF5 文件（由 save_weights 创建）中加载权重。默认情况下，模型的结构应该是不变的。 如果想将权重载入不同的模型（部分层相同）， 设置 by_name=True 来载入那些名字相同的层的权重。</li>
</ul>
<ol>
<li>Model类继承<br>通过编写<code>tf.keras.Model</code>的子类来构建一个自定义模型。在<code>init</code>方法里创建 layers。在<code>call</code>方法里定义前向传播过程。在<code>call</code>中，你可以指定自定义的损失函数，通过调用<code>self.add_loss(loss_tensor)</code>。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line"></span><br><span class="line">class SimpleMLP(keras.Model):</span><br><span class="line"></span><br><span class="line">    def __init__(self, use_bn=False, use_dp=False, num_classes=10):</span><br><span class="line">        super(SimpleMLP, self).__init__(name=&apos;mlp&apos;)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.use_dp = use_dp</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.dense1 = keras.layers.Dense(32, activation=&apos;relu&apos;)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=&apos;softmax&apos;)</span><br><span class="line">        if self.use_dp:</span><br><span class="line">            self.dp = keras.layers.Dropout(0.5)</span><br><span class="line">        if self.use_bn:</span><br><span class="line">            self.bn = keras.layers.BatchNormalization(axis=-1)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        if self.use_dp:</span><br><span class="line">            x = self.dp(x)</span><br><span class="line">        if self.use_bn:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        return self.dense2(x)</span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(...)</span><br><span class="line">model.fit(...)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>在类继承模型中，模型的拓扑结构是由 Python 代码定义的（而不是网络层的静态图）。这意味着该模型的拓扑结构不能被检查或序列化。因此，以下方法和属性不适用于类继承模型：</p>
<ul>
<li><code>model.inputs</code> 和 <code>model.outputs</code>。</li>
<li><code>model.to_yaml()</code> 和 <code>model.to_json()</code>。</li>
<li><code>model.get_config()</code> 和 <code>model.save()</code>。</li>
</ul>
<ol>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers" target="_blank" rel="noopener">tf.keras.layers</a></li>
</ol>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input" target="_blank" rel="noopener">Input</a>: 定义模型的输入</li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding" target="_blank" rel="noopener">Embedding</a>: 定义嵌入层[<a href="http://frankchen.xyz/2017/12/18/How-to-Use-Word-Embedding-Layers-for-Deep-Learning-with-Keras/" target="_blank" rel="noopener">参考</a>]<ul>
<li>Keras提供了一个嵌入层，适用于文本数据的神经网络。</li>
<li>它要求输入数据是整数编码的，所以每个字都用一个唯一的整数表示。这个数据准备步骤可以使用Keras提供的Tokenizer API来执行。</li>
<li>嵌入层用随机权重进行初始化，并将学习训练数据集中所有单词的嵌入。</li>
<li><code>e = Embedding(input_dim=200, output_dim=32, input_length=50)</code>  </li>
</ul>
</li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/add" target="_blank" rel="noopener">add</a>: 将两个输出加和</li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate" target="_blank" rel="noopener">Concatenate</a>: 链接两个张量</li>
<li><a href="https://www.programcreek.com/python/example/89694/keras.layers.dot" target="_blank" rel="noopener">dot</a></li>
</ul>
<ol>
<li>自定义layser<br>可以通过编写<code>tf.keras.layers.Layer</code>的子类来创建一个自定义<code>layer</code>，该子类编写过程中需要编写下面的方法：</li>
</ol>
<ul>
<li><code>build</code>：创建层的参数。通过<code>add_weight</code>来添加权值</li>
<li><code>call</code>：定义前向传播过程。</li>
<li><code>compute_output_shape</code>：指定怎么根据输入去计算<code>layer</code>的输出<code>shape</code>。</li>
<li>layer可以通过<code>get_config</code>方法和<code>from_config</code>方法实现串行。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class MyLayer(keras.layers.Layer):</span><br><span class="line"></span><br><span class="line">  def __init__(self, output_dim, **kwargs):</span><br><span class="line">    self.output_dim = output_dim</span><br><span class="line">    super(MyLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">  def build(self, input_shape):</span><br><span class="line">    shape = tf.TensorShape((input_shape[1], self.output_dim))</span><br><span class="line">    # Create a trainable weight variable for this layer.</span><br><span class="line">    self.kernel = self.add_weight(name=&apos;kernel&apos;,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  initializer=&apos;uniform&apos;,</span><br><span class="line">                                  trainable=True)</span><br><span class="line">    # Be sure to call this at the end</span><br><span class="line">    super(MyLayer, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">  def call(self, inputs):</span><br><span class="line">    return tf.matmul(inputs, self.kernel)</span><br><span class="line"></span><br><span class="line">  def compute_output_shape(self, input_shape):</span><br><span class="line">    shape = tf.TensorShape(input_shape).as_list()</span><br><span class="line">    shape[-1] = self.output_dim</span><br><span class="line">    return tf.TensorShape(shape)</span><br><span class="line"></span><br><span class="line">  def get_config(self):</span><br><span class="line">    base_config = super(MyLayer, self).get_config()</span><br><span class="line">    base_config[&apos;output_dim&apos;] = self.output_dim</span><br><span class="line"></span><br><span class="line">  @classmethod</span><br><span class="line">  def from_config(cls, config):</span><br><span class="line">    return cls(**config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Create a model using the custom layer</span><br><span class="line">model = keras.Sequential([MyLayer(10),</span><br><span class="line">                          keras.layers.Activation(&apos;softmax&apos;)])</span><br><span class="line"></span><br><span class="line"># The compile step specifies the training configuration</span><br><span class="line">model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),</span><br><span class="line">              loss=&apos;categorical_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line"></span><br><span class="line"># Trains for 5 epochs.</span><br><span class="line">model.fit(data, targets, batch_size=32, epochs=5)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><p>LSTM</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Input, Embedding, LSTM, Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># 标题输入：接收一个含有 100 个整数的序列，每个整数在 1 到 10000 之间。</span><br><span class="line"># 注意我们可以通过传递一个 &quot;name&quot; 参数来命名任何层。</span><br><span class="line">main_input = Input(shape=(100,), dtype=&apos;int32&apos;, name=&apos;main_input&apos;)</span><br><span class="line"></span><br><span class="line"># Embedding 层将输入序列编码为一个稠密向量的序列，</span><br><span class="line"># 每个向量维度为 512。</span><br><span class="line">x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)</span><br><span class="line"></span><br><span class="line"># LSTM 层把向量序列转换成单个向量，</span><br><span class="line"># 它包含整个序列的上下文信息</span><br><span class="line">lstm_out = LSTM(32)(x)</span><br><span class="line"></span><br><span class="line"># 插入辅助损失</span><br><span class="line">auxiliary_output = Dense(1, activation=&apos;sigmoid&apos;, name=&apos;aux_output&apos;)(lstm_out)</span><br><span class="line"></span><br><span class="line">auxiliary_input = Input(shape=(5,), name=&apos;aux_input&apos;)</span><br><span class="line">x = keras.layers.concatenate([lstm_out, auxiliary_input])</span><br><span class="line"></span><br><span class="line"># 堆叠多个全连接网络层</span><br><span class="line">x = Dense(64, activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64, activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64, activation=&apos;relu&apos;)(x)</span><br><span class="line"></span><br><span class="line"># 最后添加主要的逻辑回归层</span><br><span class="line">main_output = Dense(1, activation=&apos;sigmoid&apos;, name=&apos;main_output&apos;)(x)</span><br><span class="line"></span><br><span class="line"># 定义</span><br><span class="line">model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              loss_weights=[1., 0.2])</span><br><span class="line"></span><br><span class="line"># 传递输入数组和目标数组</span><br><span class="line">model.fit([headline_data, additional_data], [labels, labels],</span><br><span class="line">          epochs=50, batch_size=32)</span><br><span class="line"></span><br><span class="line"># 根据 name 参数编译</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&#123;&apos;main_output&apos;: &apos;binary_crossentropy&apos;, &apos;aux_output&apos;: &apos;binary_crossentropy&apos;&#125;,</span><br><span class="line">              loss_weights=&#123;&apos;main_output&apos;: 1., &apos;aux_output&apos;: 0.2&#125;)</span><br><span class="line"></span><br><span class="line"># 然后使用以下方式训练：</span><br><span class="line">model.fit(&#123;&apos;main_input&apos;: headline_data, &apos;aux_input&apos;: additional_data&#125;,</span><br><span class="line">          &#123;&apos;main_output&apos;: labels, &apos;aux_output&apos;: labels&#125;,</span><br><span class="line">          epochs=50, batch_size=32)</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/65192903" target="_blank" rel="noopener">tokenizer与embedding</a></p>
</li>
<li><p>GELU</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Activation</span><br><span class="line">from keras.utils.generic_utils import get_custom_objects</span><br><span class="line"></span><br><span class="line">def custom_gelu(x):</span><br><span class="line">    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))</span><br><span class="line">get_custom_objects().update(&#123;&apos;custom_gelu&apos;: Activation(custom_gelu)&#125;)</span><br><span class="line">fit1.add(Dense(output_dim=1, activation=custom_gelu))</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://keras.io/zh/layers/recurrent/" target="_blank" rel="noopener">RNN (LSTM, GRU) 模型</a></p>
</li>
</ol>
<ul>
<li><code>lstm1, lstm_h, lstm_c = LSTM(hideen_size, return_sequences=True, return_state=True)(input)</code><br>返回lstm的每层隐状态<code>lstm1</code>,最后输出<code>lstm_h</code>,最后的单元状态<code>lstm_c</code>。</li>
</ul>
<ol>
<li><a href="https://keras.io/api/layers/recurrent_layers/bidirectional/" target="_blank" rel="noopener">Bidirenctial layer</a></li>
</ol>
<ul>
<li><code>lstm_out = Bidirectional(LSTM(10, return_sequences=True)(input))</code></li>
<li><p>也可以分开写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">forward_layer = LSTM(10, return_sequences=True)(input)</span><br><span class="line">backward_layer = LSTM(10, activation=&apos;relu&apos;, return_sequences=True,</span><br><span class="line">                       go_backwards=True)(input)</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://stackoverflow.com/questions/47923370/keras-bidirectional-lstm-seq2seq" target="_blank" rel="noopener">使用<code>return_state</code></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoder_inputs = Input(shape=(None, num_encoder_tokens))</span><br><span class="line">encoder = Bidirectional(LSTM(latent_dim, return_state=True))</span><br><span class="line">encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)</span><br><span class="line"></span><br><span class="line">state_h = Concatenate()([forward_h, backward_h])</span><br><span class="line">state_c = Concatenate()([forward_c, backward_c])</span><br><span class="line"></span><br><span class="line">encoder_states = [state_h, state_c]</span><br><span class="line"></span><br><span class="line">decoder_inputs = Input(shape=(None, num_decoder_tokens))</span><br><span class="line">decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True)</span><br><span class="line">decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><a href="https://www.sohu.com/a/407812302_500659" target="_blank" rel="noopener">mask 使用方法</a></li>
</ol>
<ul>
<li><p>Masking layer:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = keras.layers.Masking(mask_value= 0, input_shape=(time_step,feature_size))(input)</span><br><span class="line">lstm_output = keras.layers.LSTM(hidden_size, return_sequences= True)(mask)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Embedding layer:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">embed = keras.layers.Embedding(vocab_size, embedding_size, mask_zero= True)(input)</span><br><span class="line">lstm_output = keras.layers.LSTM(hidden_size, return_sequences= True)(emded)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><a href="https://keras.io/zh/optimizers/" target="_blank" rel="noopener">optimizer</a></li>
</ol>
<ul>
<li><code>tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)</code><ul>
<li><code>decay</code>: 学习率衰减</li>
</ul>
</li>
<li><p>Learning rate</p>
<ul>
<li><p>tf 2.0 <code>tf.keras.optimizers.schedules.LearningRateSchedule</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    def __init__(self, d_model, warmup_steps=4000):</span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_model = tf.cast(self.d_model, tf.float32)</span><br><span class="line"></span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">    def __call__(self, step):</span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warmup_steps ** -1.5)</span><br><span class="line"></span><br><span class="line">        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br><span class="line"></span><br><span class="line">learning_rate = CustomSchedule(200)</span><br><span class="line">custom_adam = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, </span><br><span class="line">                                 epsilon=1e-9)</span><br></pre></td></tr></table></figure>
</li>
<li><p>keras: <code>keras.callbacks.LearningRateScheduler(schedule)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import keras.backend as K</span><br><span class="line">from keras.callbacks import LearningRateScheduler</span><br><span class="line"> </span><br><span class="line">def scheduler(epoch):</span><br><span class="line">    # 每隔100个epoch，学习率减小为原来的1/10</span><br><span class="line">    if epoch % 100 == 0 and epoch != 0:</span><br><span class="line">        lr = K.get_value(model.optimizer.lr)</span><br><span class="line">        K.set_value(model.optimizer.lr, lr * 0.1)</span><br><span class="line">        print(&quot;lr changed to &#123;&#125;&quot;.format(lr * 0.1))</span><br><span class="line">    return K.get_value(model.optimizer.lr)</span><br><span class="line"> </span><br><span class="line">reduce_lr = LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(train_x, train_y, batch_size=32, epochs=300, callbacks=[reduce_lr])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><a href="https://blog.csdn.net/zzc15806/article/details/79711114" target="_blank" rel="noopener">Reduce LR On Plateau</a><br><code>keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.1, patience=10, verbose=0, mode=&#39;auto&#39;, epsilon=0.0001, cooldown=0, min_lr=0)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from keras.callbacks import ReduceLROnPlateau</span><br><span class="line">reduce_lr = ReduceLROnPlateau(monitor=&apos;val_loss&apos;, patience=10, mode=&apos;auto&apos;)</span><br><span class="line">model.fit(train_x, train_y, batch_size=32, epochs=300, validation_split=0.1, callbacks=[reduce_lr])</span><br></pre></td></tr></table></figure></li>
</ul>
</div><div class="tags"><a href="/tags/Python"><i class="fa fa-tag">Python</i></a><a href="/tags/Machine Learning"><i class="fa fa-tag">Machine Learning</i></a><a href="/tags/Tensorflow"><i class="fa fa-tag">Tensorflow</i></a><a href="/tags/Keras"><i class="fa fa-tag">Keras</i></a></div><div class="post-nav"><a class="pre" href="/2021/08/12/cat/">cat</a><a class="next" href="/2020/05/20/BERT/">BERT</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'AMlVKrsPTMdYDOomfWIdGiFC-gzGzoHsz',
  appKey:'Msy2N1GDdBCf0W3MY9fjRYHc',
  placeholder:'Comments',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://github.com/sceliay"></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/head.jpg"></a><p>当你凝视深渊的时候，深渊也在凝视着你。</p><a class="info-icon" href="https://twitter.com/yren_zj" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="yren@zhejianglab.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/sceliay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure/">Data Structure</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML4Med/">ML4Med</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/EHR/" style="font-size: 15px;">EHR</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/tags/Theano/" style="font-size: 15px;">Theano</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/github/" style="font-size: 15px;">github</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/Pandas/" style="font-size: 15px;">Pandas</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/quality/" style="font-size: 15px;">quality</a> <a href="/tags/Daily/" style="font-size: 15px;">Daily</a> <a href="/tags/Keras/" style="font-size: 15px;">Keras</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/26/med-bert/">Med BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/18/mldl/">Lable Distribution for Multimodal Machine Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/12/cat/">cat</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/03/13/keras/">Keras 学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/20/BERT/">BERT</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/26/offer/">剑指offer</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/23/matplotlib/">Matplotlib绘图</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/22/ai4md/">AI for Medicine</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/01/ml/">Machine Learning 面试题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/30/tensorflow-2-0/">tensorflow-2.x</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Sceliay's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>